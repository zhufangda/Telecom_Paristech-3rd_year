{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "### Ali Taylan Cemgil, Bogazici University\n",
    "\n",
    "The latex equations on the Github version do not render well. Please work on a clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:26.845056Z",
     "start_time": "2018-10-12T16:41:26.838210Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "#import pygraphviz\n",
    "import pyparsing\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from IPython.display import Math\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:27.163031Z",
     "start_time": "2018-10-12T16:41:26.850016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x180 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def makeDBN(inter, intra, T, labels):\n",
    "    \"\"\"Unfold a graph for T time slices\"\"\"\n",
    "    N = max(max([i for i,j in inter]),max([j for i,j in inter]))+1\n",
    "\n",
    "    G = np.zeros((N*T,N*T))\n",
    "    pos = []\n",
    "    all_labels = []\n",
    "    for n in range(N):\n",
    "        pos.append((0,-n))\n",
    "        all_labels.append('$'+labels[n]+'_{'+str(0+1)+\"}\"+'$')\n",
    "        \n",
    "    for e in inter:\n",
    "        s,d = e\n",
    "        G[s,d] = 1\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for n in range(N):\n",
    "            pos.append((t,-n))\n",
    "            all_labels.append('$'+labels[n]+'_{'+str(t+1)+\"}\"+'$')\n",
    "\n",
    "        for e in inter:\n",
    "            s,d = e\n",
    "            s = s + N*t\n",
    "            d = d + N*t\n",
    "            G[s,d] = 1\n",
    "        \n",
    "        for e in intra:\n",
    "            s,d = e\n",
    "            s = s + N*(t-1)\n",
    "            d = d + N*t\n",
    "            G[s,d] = 1\n",
    "    return G,pos,all_labels\n",
    "\n",
    "#inter = [(0,1),(1,2),(2,3)]\n",
    "#intra = [(0,0),(1,1),(0,1),(0,2)]\n",
    "#variable_names = [\"r\",\"z\",\"x\", \"y\"] \n",
    "inter = [(0,1)]\n",
    "intra = [(0,0)]\n",
    "variable_names = [\"x\", \"y\"] \n",
    "T = 5\n",
    "\n",
    "A, pos, label_list = makeDBN(inter, intra, T, variable_names)\n",
    "\n",
    "G = nx.DiGraph(A)\n",
    "labels = {i: s for i,s in enumerate(label_list)}\n",
    "plt.figure(figsize=(12,2.5))\n",
    "nx.draw(G, pos, node_color=\"white\", node_size=2500, labels=labels, font_size=24, arrows=True)\n",
    "#nx.draw_graphviz(G,node_size=500, labels=labels, font_size=24, arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}) & = & \\sum_{x_{1:K}} p(y_{1:K}|x_{1:K}) p(x_{1:K}) \\\\\n",
    "& = &  \\underbrace{\\sum_{x_K} p(y_K | x_K ) \\sum_{x_{K-1}} p(x_K|x_{K-1})  \\dots \\sum_{x_{2}} p(x_3|x_{2})\n",
    "\\underbrace{ p(y_{2}|x_{2}) \\overbrace{ \\sum_{x_{1}} p(x_2|x_{1})\n",
    "\\underbrace{ p(y_{1}|x_{1}) \\overbrace{ p(x_1)}^{\\alpha_{1|0}}}_{\\alpha_{1|1}}\n",
    "}^{\\alpha_{2|1}} }_{\\alpha_{2|2}}}_{\\alpha_{K|K}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1|0} & \\equiv & p(x_1)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{k|k} & \\equiv & p(y_{1:k}, x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{k|k-1}  & \\equiv & p(y_{1:k-1}, x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "For $k=1, 2, \\dots, K$\n",
    "\n",
    "__Predict__\n",
    "\n",
    "$k=1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1|0}(x_1) = p(x_1)\n",
    "\\end{eqnarray}\n",
    "\n",
    "$k>1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\alpha_{k|k-1}(x_k)} & = & p(y_{1:k-1}, x_k) = \\sum_{x_{k-1}} p(x_k| x_{k-1}) p(y_{1:k-1}, x_{k-1}) \\\\\n",
    "& = & \\sum_{x_{k-1}} p(x_k| x_{k-1}) { \\alpha_{k-1|k-1}(x_{k-1}) }\n",
    "\\end{eqnarray}\n",
    "\n",
    "__Update__\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\alpha_{k|k}(x_k) } & = & p(y_{1:k}, x_k) = p(y_k | x_k) p(y_{1:k-1}, x_k) \\\\\n",
    " & = & p(y_k | x_k) {\\alpha_{k|k-1}(x_k)}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\\pmb 1": "<p><strong>SyntaxError</strong>: unexpected character after line continuation character (<ipython-input-31-55257b5da2e9>, line 1)</p>\n"
    }
   },
   "source": [
    "### Backward Pass\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}) & = &  \\sum_{x_1} p(x_1) p(y_1 | x_1 )\n",
    "%underbrace{\\sum_{x_2} p(x_2|x_{1}) p(y_2 | x_2 )}_{\\beta_1}\n",
    "\\dots\n",
    "\\underbrace{ \\sum_{x_{K-1}} p(x_{K-1}|x_{K-2}) p(y_{K-1} | x_{K-1} )\n",
    "\\underbrace{ \\sum_{x_K} p(x_K|x_{K-1}) p(y_K | x_K )\n",
    "\\underbrace{{\\pmb 1}}_{\\beta_{K|K+1}}}_{\\beta_{K-1|K}}}_{\\beta_{K-2|K-1}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k+1}(x_k) & \\equiv & p(y_{k+1:K}| x_k) \\\\\n",
    "\\beta_{k|k}(x_k) & \\equiv & p(y_{k:K}| x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "For $k=K, K-1, \\dots, 1$\n",
    "\n",
    "'Postdict' : (Backward Prediction)\n",
    "\n",
    "$k=K$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_{K|K+1}(x_K) & = & \\mathbf{1} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "$k<K$ \n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k+1}(x_k) & = & p(y_{k+1:K}| x_k) = \\sum_{x_{k+1}} p(x_{k+1}| x_{k}) p(y_{k+1:K}| x_{k+1}) \\\\\n",
    "& = & \\sum_{x_{k+1}} p(x_{k+1}| x_{k}) \\beta_{k+1|k+1}(x_{k+1}) \n",
    "\\end{eqnarray}\n",
    "\n",
    "Update\n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k}(x_k)  & = & p(y_{k:K}| x_k) = p(y_k | x_k) p(y_{k+1:K}| x_k) \\\\\n",
    " & = & p(y_k | x_k) {\\beta_{k|k+1}(x_k)}\n",
    "\\end{eqnarray}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerically Stable computation of $\\log(\\sum_i \\exp (l_i ) ))$\n",
    "\n",
    "Derivation\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L & = & \\log(\\sum_i \\exp (l_i) ) \n",
    " =   \\log(\\sum_i \\exp (l_i) \\frac{\\exp(l^*)}{\\exp(l^*)} ) \\\\\n",
    "& = &  \\log( \\exp(l^*) \\sum_i \\exp (l_i - l^*) ) \\\\\n",
    "& = &  l^* + \\log( \\sum_i \\exp (l_i - l^*) )\n",
    "\\end{eqnarray}\n",
    "\n",
    "Choose $l^*  =  \\max_i l_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:27.927429Z",
     "start_time": "2018-10-12T16:41:27.920802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive evaluation  : -inf\n",
      "Numerically stable: [-1000.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_sum_exp_naive(l):\n",
    "    return np.log(np.sum(np.exp(l)))\n",
    "\n",
    "def log_sum_exp(l, axis=0):\n",
    "    l_star = np.max(l, axis=axis, keepdims=True)\n",
    "    return l_star + np.log(np.sum(np.exp(l - l_star),axis=axis,keepdims=True)) \n",
    "    \n",
    "    \n",
    "l = np.array([-1000, -10000])\n",
    "\n",
    "print('Naive evaluation  :', log_sum_exp_naive(l))\n",
    "print('Numerically stable:', log_sum_exp(l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An implementation of the forward-backward algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:28.178985Z",
     "start_time": "2018-10-12T16:41:28.172040Z"
    }
   },
   "outputs": [],
   "source": [
    "# An implementation of the forward backward algorithm\n",
    "# For numerical stability, we calculate everything in the log domain\n",
    "\n",
    "def normalize_exp(log_P, axis=None):\n",
    "    a = np.max(log_P, keepdims=True, axis=axis)\n",
    "    P = normalize(np.exp(log_P - a), axis=axis)\n",
    "    return P\n",
    "\n",
    "\n",
    "def normalize(A, axis=None):\n",
    "    Z = np.sum(A, axis=axis,keepdims=True)\n",
    "    idx = np.where(Z == 0)\n",
    "    Z[idx] = 1\n",
    "    return A/Z\n",
    "\n",
    "def randgen(pr, N=1): \n",
    "    L = len(pr)\n",
    "    return np.random.choice(range(L), size=N, replace=True, p=pr)\n",
    "\n",
    "def predict(A, lp):\n",
    "    lstar = np.max(lp)\n",
    "    return lstar + np.log(np.dot(A,np.exp(lp-lstar)))\n",
    "\n",
    "\n",
    "def postdict(A, lp):\n",
    "    lstar = np.max(lp)\n",
    "    return lstar + np.log(np.dot(np.exp(lp-lstar), A))\n",
    "\n",
    "def update(y, logB, lp):\n",
    "    return logB[y,:] + lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:28.204782Z",
     "start_time": "2018-10-12T16:41:28.184536Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate Parameter Structure\n",
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "logA = np.log(A)\n",
    "logB = np.log(B)\n",
    "\n",
    "# Generate Data\n",
    "\n",
    "# Number of steps\n",
    "T = 100\n",
    "\n",
    "x = np.zeros(T,int)\n",
    "y = np.zeros(T,int)\n",
    "for t in range(T):\n",
    "    if t==0:\n",
    "        x[t] = randgen(p)\n",
    "    else:\n",
    "        x[t] = randgen(A[:,x[t-1]])\n",
    "    \n",
    "    y[t] = randgen(B[:,x[t]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:28.219004Z",
     "start_time": "2018-10-12T16:41:28.207109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "\n",
    "# Python indices start from zero so\n",
    "# log \\alpha_{k|k} will be in log_alpha[:,k-1]\n",
    "# log \\alpha_{k|k-1} will be in log_alpha_pred[:,k-1]\n",
    "log_alpha  = np.zeros((S, T))\n",
    "log_alpha_pred = np.zeros((S, T))\n",
    "for k in range(T):\n",
    "    if k==0:\n",
    "        log_alpha_pred[:,0] = np.log(p)\n",
    "    else:\n",
    "        log_alpha_pred[:,k] = predict(A, log_alpha[:,k-1])\n",
    "    \n",
    "    log_alpha[:,k] = update(y[k], logB, log_alpha_pred[:,k])\n",
    "    \n",
    "# Backward Pass\n",
    "log_beta  = np.zeros((S, T))\n",
    "log_beta_post = np.zeros((S, T))\n",
    "\n",
    "for k in range(T-1,-1,-1):\n",
    "    if k==T-1:\n",
    "        log_beta_post[:,k] = np.zeros(S)\n",
    "    else:\n",
    "        log_beta_post[:,k] = postdict(A, log_beta[:,k+1])\n",
    "    \n",
    "    log_beta[:,k] = update(y[k], logB, log_beta_post[:,k])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing - Forward Backward Algorithm (Two filter formulation)\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}, x_k) & = & p(y_{1:k}, x_k) p(y_{k+1:K} | x_k) \\\\\n",
    "& = & {\\alpha_{k|k}(x_k) } {\\beta_{k|k+1}(x_k)} \\\\\n",
    "& \\equiv & \\gamma_k(x_k)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:28.453587Z",
     "start_time": "2018-10-12T16:41:28.448874Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434 -135.22434 -135.22434\n",
      "  -135.22434 -135.22434 -135.22434 -135.22434]]\n"
     ]
    }
   ],
   "source": [
    "# Smoother check\n",
    "# All numbers must be equal to the marginal likelihood p(y_{1:K})\n",
    "\n",
    "log_gamma = log_alpha + log_beta_post\n",
    "print(log_sum_exp(log_gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\\pmb 1": "<p><strong>SyntaxError</strong>: unexpected character after line continuation character (<ipython-input-36-55257b5da2e9>, line 1)</p>\n"
    }
   },
   "source": [
    "## Viterbi Algorithm \n",
    "\n",
    "Finding the most likely state trajectory \n",
    "\n",
    "$+ \\rightarrow \\max$, $\\times \\rightarrow +$, \n",
    "\n",
    "\\begin{eqnarray}\n",
    "x^*_{1:K} & = & \\arg\\max_{x_{1:K}} p(y_{1:K}|x_{1:K}) p(x_{1:K}) \\\\\n",
    " & = & \\arg\\max_{x_{1:K}} \\log p(y_{1:K}|x_{1:K}) + \\log p(x_{1:K}) \\\\\n",
    " & = & \\arg\\max_{x_{1:K}} L(y_{1:K}|x_{1:K}) + L(x_{1:K}) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}|x^*_{1:K}) & = & \\max_{x_{1:K}} p(y_{1:K}|x_{1:K}) p(x_{1:K}) \\\\\n",
    "& = &  \\underbrace{\\max_{x_K} L(y_T | x_K ) +\\max_{x_{K-1}} L(x_K|x_{K-1})}_{\\alpha_K}  \\dots +\\max_{x_{2}} L(x_3|x_{2})\n",
    "                               \\underbrace{L(y_{2}|x_{2})\\overbrace{ +\\max_{x_{1}} L(x_2|x_{1})}^{\\alpha_{2|1}} }_{\\alpha_2}\n",
    "                                 \\underbrace{L(y_{1}|x_{1})+\\overbrace{L(x_1)}^{\\alpha_{1|0}}}_{\\alpha_1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}|x^*_{1:K})  =   \\max_{x_1} p(x_1) p(y_1 | x_1 )\n",
    "%underbrace{\\max_{x_2} p(x_2|x_{1}) p(y_2 | x_2 )}_{\\beta_1}\n",
    "\\dots\n",
    "\\underbrace{ \\max_{x_{K-1}} p(x_{K-1}|x_{K-2}) p(y_{K-1} | x_{K-1} )}_{\\beta_{K-2}}\n",
    "\\underbrace{ \\max_{x_K} p(x_K|x_{K-1}) p(y_K | x_K )}_{\\beta_{K-1}}\n",
    "\\underbrace{{\\pmb 1}}_{\\beta_{K}}\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:28.796283Z",
     "start_time": "2018-10-12T16:41:28.785095Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 2 1 1 2 1 1 0 2 1 1 2 2 1 2 1 1 1 2 1 1 0 2 1 1 2 2 1 1 1 2 2 2 2 1 2\n",
      " 1 2 1 1 2 1 0 2 1 1 1 0 2 2 2 1 2 1 1 1 1 2 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1\n",
      " 2 1 1 1 1 2 1 2 1 2 1 2 2 1 1 1 2 1 2 1 2 1 2 1 2 1]\n",
      "[0 2 1 2 1 2 1 1 1 2 1 1 2 2 1 2 1 1 1 2 1 1 2 1 0 0 2 2 1 1 1 0 2 2 1 1 2\n",
      " 1 2 1 1 2 1 0 2 1 1 1 1 1 1 2 1 2 1 1 1 1 2 1 1 1 0 2 1 1 1 1 2 1 1 1 1 1\n",
      " 2 1 1 1 1 2 1 0 2 2 1 0 2 1 1 1 2 1 0 2 2 1 2 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "def predict_maxm(log_A, lp):\n",
    "    return np.max(log_A + lp, axis=1)\n",
    "\n",
    "def postdict_maxm(log_A, lp):\n",
    "    return np.max(log_A.T + lp, axis=1)\n",
    "\n",
    "# Forward Pass\n",
    "\n",
    "# Python indices start from zero so\n",
    "# log \\alpha_{k|k} will be in log_alpha[:,k-1]\n",
    "# log \\alpha_{k|k-1} will be in log_alpha_pred[:,k-1]\n",
    "log_alpha  = np.zeros((S, T))\n",
    "log_alpha_pred = np.zeros((S, T))\n",
    "for k in range(T):\n",
    "    if k==0:\n",
    "        log_alpha_pred[:,0] = np.log(p)\n",
    "    else:\n",
    "        log_alpha_pred[:,k] = predict_maxm(logA, log_alpha[:,k-1])\n",
    "    \n",
    "    log_alpha[:,k] = update(y[k], logB, log_alpha_pred[:,k])\n",
    "    \n",
    "# Backward Pass\n",
    "log_beta  = np.zeros((S, T))\n",
    "log_beta_post = np.zeros((S, T))\n",
    "\n",
    "for k in range(T-1,-1,-1):\n",
    "    if k==T-1:\n",
    "        log_beta_post[:,k] = np.zeros(S)\n",
    "    else:\n",
    "        log_beta_post[:,k] = postdict_maxm(logA, log_beta[:,k+1])\n",
    "    \n",
    "    log_beta[:,k] = update(y[k], logB, log_beta_post[:,k])\n",
    "    \n",
    "log_delta = log_alpha + log_beta_post\n",
    "print(np.argmax(log_delta,axis=0))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:28.811079Z",
     "start_time": "2018-10-12T16:41:28.799276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-159.40451 -159.0681  -156.8051 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.83826, -2.32904, -1.813  ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(log_alpha[:,-2])\n",
    "log_beta_post[:,-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing (Forward filtering - Backward smoothing), The Correction Smoother\n",
    "\n",
    "Suppose we have computed the filtered quantities $p(x_t| y_{1:t})$ via the forward pass. The forward-backward algorithm requires storing all the observations. In a batch settings, storing the observations and filtering densities may be OK however when observations are arriving indeed sequentially, this may be not desired.  \n",
    "\n",
    "We will derive a recursive algorithm to compute the marginals $p(x_t | y_{1:T})$.\n",
    "\n",
    "Note that if we calculate instead the so-called __pairwise__ marginal $p(x_t, x_{t+1} | y_{1:T} )$, we can get each marginal simply by: \n",
    "\\begin{align}\n",
    "p(x_t | y_{1:T}) & =  \\sum_{x_{t+1}} p(x_t, x_{t+1} | y_{1:T} )  & \\text{Definition} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "p(x_t, x_{t+1} | y_{1:T} ) & =   p(x_{t} |x_{t+1}, y_{1:t},y_{t+1:T} ) p(x_{t+1}|y_{1:T} ) & \\text{Factorize} \\\\\n",
    "& =   p(x_{t} |x_{t+1}, y_{1:t} ) p(x_{t+1}|y_{1:T} ) & \\text{Conditional Independence}\\\\\n",
    "  & =  \\frac{p(x_{t}, x_{t+1}| y_{1:t} )}{p(x_{t+1}| y_{1:t} )} p(x_{t+1}|y_{1:T} ) & \\text{Definition of Conditional} \n",
    "\\end{align}\n",
    "\n",
    "This update has the form:\n",
    "\\begin{eqnarray}\n",
    "\\text{New Pairwise Marginal}_{t,t+1} & = & \\frac{\\text{Old Pairwise Marginal}_{t,t+1}}{\\text{Old Marginal}_{t+1}} \\times {\\text{New Marginal}_{t+1}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "The old pairwise marginal can be simply calculated from the filtered marginals as\n",
    "\n",
    "\\begin{align}\n",
    "p(x_{t}, x_{t+1}| y_{1:t} ) & =  p(x_{t+1} | x_t,  y_{1:t} ) p(x_t | y_{1:t}) & \\text{Definition} \\\\\n",
    "& =  p(x_{t+1} | x_t ) p(x_t | y_{1:t}) & \\text{Conditional Independence} \\\\\n",
    "& = \\text{Transition Model} \\times \\text{Filtering distribution} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "The correction smoother calculates a factorisation of the posterior of form\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(x_{1:T}|y_{1:T}) & = & \\frac{\\prod_{t=1}^{T-1} p(x_{t}, x_{t+1} | y_{1:T}) }{ \\prod_{t=2}^{T-1} p(x_{t} | y_{1:T}) }\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:29.307463Z",
     "start_time": "2018-10-12T16:41:29.066446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correction Smoother\n",
    "# For numerical stability, we calculate everything in the log domain\n",
    "log_gamma_corr = np.zeros_like(log_alpha)\n",
    "log_gamma_corr[:,T-1] = log_alpha[:,T-1]\n",
    "\n",
    "for k in range(T-2,-1,-1):\n",
    "    log_old_pairwise_marginal = log_alpha[:,k].reshape(1,S) + logA \n",
    "    log_old_marginal = predict(A, log_alpha[:,k])\n",
    "    log_new_pairwise_marginal = log_old_pairwise_marginal + log_gamma_corr[:,k+1].reshape(S,1) - log_old_marginal.reshape(S,1)\n",
    "    log_gamma_corr[:,k] = log_sum_exp(log_new_pairwise_marginal, axis=0).reshape(S)\n",
    "    \n",
    "\n",
    "# Verify that result coincide\n",
    "\n",
    "gam = normalize_exp(log_gamma, axis=0)\n",
    "gam_corr = normalize_exp(log_gamma_corr, axis=0)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(gam, interpolation='nearest')\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(gam_corr, interpolation='nearest')\n",
    "\n",
    "plt.show()\n",
    "#print(log_gamma)\n",
    "#print(log_gamma_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of the HMM in Python\n",
    "\n",
    "We will integrate filtering, smoothing and training functionality into an HMM object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:29.406797Z",
     "start_time": "2018-10-12T16:41:29.367203Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notes_utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-51b1021b74d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# %load HiddenMarkovModel.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnotes_utilities\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_sum_exp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize_exp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'notes_utilities'"
     ]
    }
   ],
   "source": [
    "# %load HiddenMarkovModel.py\n",
    "import numpy as np\n",
    "from notes_utilities import randgen, log_sum_exp, normalize_exp, normalize\n",
    "\n",
    "class HMM(object):\n",
    "    def __init__(self, pi, A, B):\n",
    "        # p(x_0)\n",
    "        self.pi = pi\n",
    "        # p(x_k|x_{k-1})\n",
    "        self.A = A\n",
    "        # p(y_k|x_{k})\n",
    "        self.B = B\n",
    "        # Number of possible latent states at each time\n",
    "        self.S = pi.shape[0]\n",
    "        # Number of possible observations at each time\n",
    "        self.R = B.shape[0]\n",
    "        self.logB = np.log(self.B)\n",
    "        self.logA = np.log(self.A)\n",
    "        self.logpi = np.log(self.pi)\n",
    "    \n",
    "    def set_param(self, pi=None, A=None, B=None):\n",
    "        if pi is not None:\n",
    "            self.pi = pi\n",
    "            self.logpi = np.log(self.pi)\n",
    "\n",
    "        if A is not None:\n",
    "            self.A = A\n",
    "            self.logA = np.log(self.A)\n",
    "\n",
    "        if B is not None:\n",
    "            self.B = B\n",
    "            self.logB = np.log(self.B)\n",
    "\n",
    "    @classmethod\n",
    "    def from_random_parameters(cls, S=3, R=5):\n",
    "        A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "        B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "        pi = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "        return cls(pi, A, B)\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = \"Prior:\\n\" + str(self.pi) + \"\\nA:\\n\" + str(self.A) + \"\\nB:\\n\" + str(self.B)\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.__str__()\n",
    "        return s\n",
    "\n",
    "    def predict(self, lp):\n",
    "        lstar = np.max(lp)\n",
    "        return lstar + np.log(np.dot(self.A,np.exp(lp-lstar)))\n",
    "\n",
    "    def postdict(self, lp):\n",
    "        lstar = np.max(lp)\n",
    "        return lstar + np.log(np.dot(np.exp(lp-lstar), self.A))\n",
    "\n",
    "    def predict_maxm(self, lp):\n",
    "        return np.max(self.logA + lp, axis=1)\n",
    "\n",
    "    def postdict_maxm(self, lp):\n",
    "        return np.max(self.logA.T + lp, axis=1)\n",
    "\n",
    "    def update(self, y, lp):\n",
    "        return self.logB[y,:] + lp if not np.isnan(y) else lp\n",
    "\n",
    "    def generate_sequence(self, T=10):\n",
    "    # T: Number of steps\n",
    "\n",
    "        x = np.zeros(T, int)\n",
    "        y = np.zeros(T, int)\n",
    "\n",
    "        for t in range(T):\n",
    "            if t==0:\n",
    "                x[t] = randgen(self.pi)\n",
    "            else:\n",
    "                x[t] = randgen(self.A[:,x[t-1]])    \n",
    "            y[t] = randgen(self.B[:,x[t]])\n",
    "    \n",
    "        return y, x\n",
    "\n",
    "    def forward(self, y, maxm=False):\n",
    "        T = len(y)\n",
    "        \n",
    "        # Forward Pass\n",
    "\n",
    "        # Python indices start from zero so\n",
    "        # log \\alpha_{k|k} will be in log_alpha[:,k-1]\n",
    "        # log \\alpha_{k|k-1} will be in log_alpha_pred[:,k-1]\n",
    "        log_alpha  = np.zeros((self.S, T))\n",
    "        log_alpha_pred = np.zeros((self.S, T))\n",
    "        for k in range(T):\n",
    "            if k==0:\n",
    "                log_alpha_pred[:,0] = self.logpi\n",
    "            else:\n",
    "                if maxm:\n",
    "                    log_alpha_pred[:,k] = self.predict_maxm(log_alpha[:,k-1])\n",
    "                else:\n",
    "                    log_alpha_pred[:,k] = self.predict(log_alpha[:,k-1])\n",
    "\n",
    "                \n",
    "            log_alpha[:,k] = self.update(y[k], log_alpha_pred[:,k])\n",
    "            \n",
    "        return log_alpha, log_alpha_pred\n",
    "            \n",
    "    def backward(self, y, maxm=False):\n",
    "        # Backward Pass\n",
    "        T = len(y)\n",
    "        log_beta  = np.zeros((self.S, T))\n",
    "        log_beta_post = np.zeros((self.S, T))\n",
    "\n",
    "        for k in range(T-1,-1,-1):\n",
    "            if k==T-1:\n",
    "                log_beta_post[:,k] = np.zeros(self.S)\n",
    "            else:\n",
    "                if maxm: \n",
    "                    log_beta_post[:,k] = self.postdict_maxm(log_beta[:,k+1])                    \n",
    "                else:\n",
    "                    log_beta_post[:,k] = self.postdict(log_beta[:,k+1])\n",
    "\n",
    "            log_beta[:,k] = self.update(y[k], log_beta_post[:,k])\n",
    "\n",
    "        return log_beta, log_beta_post\n",
    "        \n",
    "    def forward_backward_smoother(self, y):\n",
    "        log_alpha, log_alpha_pred = self.forward(y)\n",
    "        log_beta, log_beta_post = self.backward(y)\n",
    "        \n",
    "        log_gamma = log_alpha + log_beta_post\n",
    "        return log_gamma\n",
    "\n",
    "    def viterbi(self, y):\n",
    "        T = len(y)\n",
    "        \n",
    "        # Forward Pass\n",
    "        log_alpha  = np.zeros((self.S, T))\n",
    "        for k in range(T):\n",
    "            if k==0:\n",
    "                log_alpha_pred = self.logpi\n",
    "            else:\n",
    "                log_alpha_pred = self.predict(log_alpha[:,k-1])\n",
    "                \n",
    "            log_alpha[:,k] = self.update(y[k], log_alpha_pred)\n",
    "\n",
    "        xs = list()\n",
    "        w = np.argmax(log_alpha[:,-1])\n",
    "        xs.insert(0, w)                \n",
    "        for k in range(T-2,-1,-1):\n",
    "            w = np.argmax(log_alpha[:,k] + self.logA[w,:])\n",
    "            xs.insert(0, w)                \n",
    "            \n",
    "        return xs\n",
    "            \n",
    "    def viterbi_maxsum(self, y):\n",
    "        '''Vanilla implementation of Viterbi decoding via max-sum'''\n",
    "        '''This algorithm may fail to find the MAP trajectory as it breaks ties arbitrarily'''\n",
    "        log_alpha, log_alpha_pred = self.forward(y, maxm=True)\n",
    "        log_beta, log_beta_post = self.backward(y, maxm=True)\n",
    "        \n",
    "        log_delta = log_alpha + log_beta_post\n",
    "        return np.argmax(log_delta, axis=0)\n",
    "\n",
    "    \n",
    "    def correction_smoother(self, y):\n",
    "        # Correction Smoother\n",
    "\n",
    "        log_alpha, log_alpha_pred = self.forward(y)\n",
    "        T = len(y)\n",
    "        \n",
    "        # For numerical stability, we calculate everything in the log domain\n",
    "        log_gamma_corr = np.zeros_like(log_alpha)\n",
    "        log_gamma_corr[:,T-1] = log_alpha[:,T-1]\n",
    "\n",
    "        C2 = np.zeros((self.S, self.S))\n",
    "        C3 = np.zeros((self.R, self.S))\n",
    "        C3[y[-1],:] = normalize_exp(log_alpha[:,T-1])\n",
    "        for k in range(T-2,-1,-1):\n",
    "            log_old_pairwise_marginal = log_alpha[:,k].reshape(1,self.S) + self.logA \n",
    "            log_old_marginal = self.predict(log_alpha[:,k])\n",
    "            log_new_pairwise_marginal = log_old_pairwise_marginal + log_gamma_corr[:,k+1].reshape(self.S,1) - log_old_marginal.reshape(self.S,1)\n",
    "            log_gamma_corr[:,k] = log_sum_exp(log_new_pairwise_marginal, axis=0).reshape(self.S)\n",
    "            C2 += normalize_exp(log_new_pairwise_marginal)\n",
    "            C3[y[k],:] += normalize_exp(log_gamma_corr[:,k])\n",
    "        C1 = normalize_exp(log_gamma_corr[:,0])\n",
    "        return log_gamma_corr, C1, C2, C3\n",
    "    \n",
    "    def forward_only_SS(self, y, V=None):\n",
    "        # Forward only estimation of expected sufficient statistics\n",
    "        T = len(y)\n",
    "        \n",
    "        if V is None:\n",
    "            V1  = np.eye((self.S))\n",
    "            V2  = np.zeros((self.S,self.S,self.S))\n",
    "            V3  = np.zeros((self.R,self.S,self.S))\n",
    "        else:\n",
    "            V1, V2, V3 = V\n",
    "            \n",
    "        I_S1S = np.eye(self.S).reshape((self.S,1,self.S))\n",
    "        I_RR = np.eye(self.R)\n",
    "        \n",
    "        for k in range(T):\n",
    "            if k==0:\n",
    "                log_alpha_pred = self.logpi\n",
    "            else:\n",
    "                log_alpha_pred = self.predict(log_alpha)\n",
    "\n",
    "            if k>0:\n",
    "                #print(self.S, self.R)\n",
    "                #print(log_alpha)\n",
    "                # Calculate p(x_{k-1}|y_{1:k-1}, x_k) \n",
    "                lp = np.log(normalize_exp(log_alpha)).reshape(self.S,1) + self.logA.T    \n",
    "                P = normalize_exp(lp, axis=0)\n",
    "\n",
    "                # Update\n",
    "                V1 = np.dot(V1, P)             \n",
    "                V2 = np.dot(V2, P) + I_S1S*P.reshape((1,self.S,self.S))    \n",
    "                V3 = np.dot(V3, P) + I_RR[:,y[k-1]].reshape((self.R,1,1))*P.reshape((1,self.S,self.S))    \n",
    "\n",
    "            log_alpha = self.update(y[k], log_alpha_pred)    \n",
    "            p_xT = normalize_exp(log_alpha)    \n",
    "\n",
    "        C1 = np.dot(V1, p_xT.reshape(self.S,1))\n",
    "        C2 = np.dot(V2, p_xT.reshape(1,self.S,1)).reshape((self.S,self.S))\n",
    "        C3 = np.dot(V3, p_xT.reshape(1,self.S,1)).reshape((self.R,self.S))\n",
    "        C3[y[-1],:] +=  p_xT\n",
    "        \n",
    "        ll = log_sum_exp(log_alpha)\n",
    "        \n",
    "        return C1, C2, C3, ll, (V1, V2, V3)\n",
    "\n",
    "    def train_EM(self, y, EPOCH=10):\n",
    "        \n",
    "        LL = np.zeros(EPOCH)\n",
    "        for e in range(EPOCH):\n",
    "            C1, C2, C3, ll, V = self.forward_only_SS(y)\n",
    "            LL[e] = ll\n",
    "            p = normalize(C1 + 0.1, axis=0).reshape(self.S)\n",
    "            #print(p,np.size(p))            \n",
    "            A = normalize(C2, axis=0)\n",
    "            #print(A)\n",
    "            B = normalize(C3, axis=0)\n",
    "            #print(B)\n",
    "            self.__init__(p, A, B)\n",
    "            # print(ll)\n",
    "            \n",
    "        return LL\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:29.407115Z",
     "start_time": "2018-10-12T16:41:29.372Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from HiddenMarkovModel import *\n",
    "    \n",
    "hm = HMM.from_random_parameters()\n",
    "\n",
    "y,x = hm.generate_sequence(300)\n",
    "\n",
    "log_alpha, log_alpha_pred = hm.forward(y)\n",
    "log_gamma = hm.forward_backward_smoother(y)\n",
    "log_gamma_corr, C1_corr, C2_corr, C3_corr = hm.correction_smoother(y)\n",
    "C1, C2, C3, ll, V = hm.forward_only_SS(y)\n",
    "\n",
    "print(C2)\n",
    "print(C2_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:29.409184Z",
     "start_time": "2018-10-12T16:41:29.379Z"
    }
   },
   "outputs": [],
   "source": [
    "l = list()\n",
    "\n",
    "l.insert(0,3)\n",
    "l.insert(0,2)\n",
    "l.insert(0,13)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:29.411164Z",
     "start_time": "2018-10-12T16:41:29.386Z"
    }
   },
   "outputs": [],
   "source": [
    "LL = hm.train_EM(y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:29.412103Z",
     "start_time": "2018-10-12T16:41:29.393Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(LL)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example 1\n",
    "\n",
    "A Robot moving around of a circular corridor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:29.709877Z",
     "start_time": "2018-10-12T16:41:29.688704Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HMM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-831666e9b421>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mp0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mhm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HMM' is not defined"
     ]
    }
   ],
   "source": [
    "# # Number of states\n",
    "# S = 50\n",
    "\n",
    "# # Probability of staying on the same tile\n",
    "# ep = 0.7\n",
    "# # Probability of making an arbitrary jump\n",
    "# kidnap = 0.01\n",
    "# # Probability of correct observation\n",
    "# a = 0.9\n",
    "\n",
    "# Set up the transition matrix\n",
    "idx = [i for i in range(1,S)]+[0]\n",
    "I = np.diag(np.ones(S))\n",
    "A2 = (1-kidnap)*(ep*I + (1-ep)*I[:,idx]) + kidnap*np.ones((S,S))/S\n",
    "kidnap = 0.0\n",
    "A = (1-kidnap)*(ep*I + (1-ep)*I[:,idx]) + kidnap*np.ones((S,S))/S\n",
    "\n",
    "# Set up the observation matrix\n",
    "c = a*np.random.randint(0,2, S) + (1-a)*np.ones(S)/2\n",
    "C = np.array([c,1-c])\n",
    "\n",
    "# Prior\n",
    "p0 = np.ones(S)/S\n",
    "#p0 = np.random.rand(S)\n",
    "p0 = p0/sum(p0)\n",
    "\n",
    "hm2 = HMM(p0, A2, C)\n",
    "\n",
    "T = 500\n",
    "y,x = hm2.generate_sequence(T)\n",
    "xs = list()\n",
    "hm = HMM(p0, A, C)\n",
    "\n",
    "\n",
    "\n",
    "log_alpha, log_alpha_pred = hm.forward(y)\n",
    "log_gamma = hm.forward_backward_smoother(y)\n",
    "xs = hm.viterbi(y)\n",
    "\n",
    "alpha = normalize_exp(log_alpha, axis=0)\n",
    "alpha_pred = normalize_exp(log_alpha_pred, axis=0)\n",
    "gam = normalize_exp(log_gamma, axis=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(hm.S//5,1))\n",
    "plt.imshow(C[1,:].reshape(1,hm.S), interpolation='nearest', cmap='gray_r')\n",
    "ax = plt.gca()\n",
    "ax.set_yticks([])\n",
    "ax.invert_yaxis()\n",
    "plt.title('Probability of observing a black tile at state x')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.imshow(y.reshape(1,len(y)), interpolation='nearest', cmap='gray_r')\n",
    "ax = plt.gca()\n",
    "ax.set_yticks([])\n",
    "ax.invert_yaxis()\n",
    "plt.title('Observations')\n",
    "plt.xlabel('time')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.imshow(alpha, interpolation='nearest', cmap='gray_r')\n",
    "ax = plt.gca()\n",
    "ax.invert_yaxis()\n",
    "plt.title('Filtered marginals')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.imshow(alpha_pred, interpolation='nearest', cmap='gray_r')\n",
    "ax = plt.gca()\n",
    "ax.invert_yaxis()\n",
    "plt.title('Predicted marginals')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.imshow(gam, interpolation='nearest', cmap='gray_r')\n",
    "plt.plot(xs,'wo')\n",
    "plt.plot(x,'.')\n",
    "ax = plt.gca()\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlim((-0.5,T-0.5))\n",
    "ax.set_ylim((-0.5,S-0.5))\n",
    "plt.title('Smoothed marginals')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example 2\n",
    "\n",
    "Positioning on a lake with a depth sensor.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\\ind{i = x_t": "<p><strong>SyntaxError</strong>: unexpected character after line continuation character (<ipython-input-41-26cafc07e22b>, line 1)</p>\n",
     "\\ind{s = x_t": "<p><strong>SyntaxError</strong>: unexpected character after line continuation character (<ipython-input-41-7d360a036a13>, line 1)</p>\n"
    }
   },
   "source": [
    "\n",
    "### Prelude: Estimating parameters of an homogeneous Markov chain\n",
    "$\\newcommand{\\ind}[1]{\\left[{#1}\\right]}$\n",
    "\n",
    "We have a Markov chain with transition probabilities $p(x_t = i| x_{t-1} = j) =  A_{i,j}$\n",
    "and initial state $p(x_1) = \\pi_i$.\n",
    "\n",
    "The distributions are\n",
    "\\begin{eqnarray}\n",
    "p(x_1 |\\pi)& = &\\prod_{s=1}^{S} \\pi_s^{\\ind{s = x_1}} \\\\\n",
    "p(x_t | x_{t-1}, A) &=& \\prod_{j=1}^{S} \\prod_{s=1}^{S}  A_{s,j}^{{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "The loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A) & = & \\log \\left( p(x_1 | \\pi) \\prod_{t=2}^T p(x_t | x_{t-1}, A) \\right) \\\\\n",
    "& = & \\sum_{s=1}^{S} {\\ind{s = x_1}} \\log \\pi_s + \\sum_{t=2}^T \\sum_{j=1}^{S}\\sum_{s=1}^{S} {{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\log A_{s,j}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We have the constraints $\\sum_s \\pi_s = 1$ and $\\sum_i A_{i,j} = 1$ for all $j=1 \\dots S$ so we have $S+1$ constraints. We write the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A) & = & \\sum_{s=1}^{S} {\\ind{s = x_1}} \\log \\pi_s + \\sum_{t=2}^T \\sum_{j=1}^{S} \\sum_{s=1}^{S} {{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\log A_{s,j} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "To find $\\pi$ and $A$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A)}{\\partial \\pi_i} & = & {\\ind{i = x_1}} \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)}{\\partial A_{i,j}} & = & \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{A_{i,j}} - \\lambda^A_j = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "Substitute the constraints $\\sum_s \\pi_s = 1$ and $\\sum_s A_{s,j} = 1, \\; j=1\\dots S$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & {\\ind{i = x_1}} \\frac{1}{\\lambda^\\pi} \\\\\n",
    "\\sum_i \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i {\\ind{i = x_1}} = 1\\\\\n",
    "\\lambda^\\pi & = & 1\\\\\n",
    "\\pi_i & = & {\\ind{i = x_1}}\n",
    "\\end{eqnarray}\n",
    "As we have effectively only a single observation for $x_1$, we have a crisp estimate.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_{t=2}^T \\sum_i  {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\\\\n",
    "A_{i,j} & = & \\frac{\\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}{\\sum_{t=2}^T \\sum_i  {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}\\\\\n",
    "& = & \\frac{\\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}{\\sum_{t=2}^T \\ind{j = x_{t-1}}}\n",
    "\\end{eqnarray}\n",
    "The result is intuitive. The denominator counts the number of times the chain visited state $j$ in the previous state. The numerator counts the number of times we visit $i$ given we were at $j$ the previous time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating parameters of an homogeneous Markov chain when several sequences are observed\n",
    "\n",
    "Suppose we have observed several sequences\n",
    "\\begin{eqnarray}\n",
    "X = \\{x_1^{(n)}, x_2^{(n)}, \\dots, x_{T_n}^{(n)}   \\}\n",
    "\\end{eqnarray}\n",
    "for $n = 1\\dots N$. Here $T_n$ is the length of the $n$'th sequence.\n",
    "\n",
    "The notation becomes slightly more complicated but conceptully the derivation is similar.\n",
    "\n",
    "The joint probability of all sequences is\n",
    "\\begin{eqnarray}\n",
    "p(X | \\pi, A) & = & \\prod_n \\left( p(x_1^{(n)}) \\prod_{t=2}^{T_n} p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right)\n",
    "\\end{eqnarray}\n",
    "The loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A) & = & \\sum_n \\left( \\log p(x_1^{(n)}) + \\sum_{t=2}^{T_n} \\log p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right) \\\\\n",
    "& = & \\sum_n \\left( \\sum_{s=1}^{S} {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_{t=2}^{T_n}\\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S} {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} \\right) \\\\\n",
    "& = & \\sum_{s=1}^{S} \\sum_n  {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_n \\sum_{t=2}^{T_n}\n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S}   {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "We write the Lagrangian\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)\n",
    "& = & \\sum_{s=1}^{S} \\sum_n  {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_n  \\sum_{t=2}^{T_n} \\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S}  {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\\ind{i = x_t^{(n)": "<p><strong>SyntaxError</strong>: unexpected character after line continuation character (<ipython-input-41-ea1dee45a3b2>, line 1)</p>\n"
    }
   },
   "source": [
    "To find $\\pi$ and $A$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A)}{\\partial \\pi_i} & = & \\sum_n {\\ind{i = x_1^{(n)}}} \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)}{\\partial A_{i,j}} & = &  \\sum_n \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{A_{i,j}} - \\lambda^A_j = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Prior\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\sum_n {\\ind{i = x_1^{(n)}}} \\frac{1}{\\lambda^\\pi}\\\\\n",
    "\\sum \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i \\sum_n {\\ind{i = x_1^{(n)}}} = 1 \\\\\n",
    "\\lambda^\\pi & = & \\sum_i \\sum_n {\\ind{i = x_1^{(n)}}} = N \\\\\n",
    "\\pi_i & = & \\frac{1}{N} \\sum_n {\\ind{i = x_1^{(n)}}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Transition Matrix\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & \\sum_n \\sum_{t=2}^{T_n}  {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i \\sum_n \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_i \\sum_n  \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\\\\n",
    "& = &  \\sum_n \\sum_{t=2}^{T_n}  \\ind{j = x_{t-1}^{(n)}} \\\\\n",
    "A_{i,j} & = &  \\frac{\\sum_n \\sum_{t=2}^{T_n}  {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}}}{\\sum_n \\sum_{t=2}^{T_n}  {\\ind{j = x_{t-1}^{(n)}}}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The EM Algorithm\n",
    "\n",
    "$\\newcommand{\\E}[1]{\\left\\langle#1\\right\\rangle}$\n",
    "\n",
    "The EM algorithm is a standart approach for ML estimation, when we have hidden variables.\n",
    "The canonical model is $p(y, x| \\theta)$ where we observe only $y$.\n",
    "\n",
    "The observed data loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & = & \\log p(y| \\theta) = \\log \\sum_x p(y, x| \\theta)\n",
    "\\end{eqnarray}\n",
    "\n",
    "The key to the EM algorithm is the Jensen's inequality, that states for a concave function $f$ we have for $0 \\leq \\lambda \\leq 1$\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "f(\\lambda x_1 + (1 - \\lambda) x_2) \\geq \\lambda f( x_1) + (1 - \\lambda) f(x_2)\n",
    "\\end{eqnarray}\n",
    "\n",
    "In words the value of a function evaluated at the convex combination (lhs) is always equal and larger then the convex combination of the function values. As mathematical expectation\n",
    "\\begin{eqnarray}\n",
    "\\E{f(x)} & = & \\sum_x p(x) f(x) \\\\\n",
    "\\sum_x p(x) & = & 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "As $\\log(x)$  is a concave function, we have\n",
    "\n",
    "\\begin{eqnarray}\n",
    " f(\\E{x}) & \\geq & \\E{f(x)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The key idea of the EM algorithm is to lower bound the observed data likelihood an maximize the bound with respect to the parameters. We take any distribution $q(x)$\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & = & \\log \\sum_x p(y, x| \\theta) \\\\\n",
    "& = & \\log \\sum_x p(y, x| \\theta) \\frac{q(x)}{q(x)} \\\\\n",
    "& = & \\log  \\E{\\frac{p(y, x| \\theta)}{q(x)}}_{q(x)}\\\\\n",
    "& \\geq & \\E{\\log {p(y, x| \\theta)}}_{q(x)} -\\E{\\log{q(x)} }_{q(x)}\\\\\n",
    "\\end{eqnarray}\n",
    "For _any_ $q(x)$, we have a lower bound. The natural strategy here is to choose the $q(x)$ that will maximise the lower bound. This is an optimisation problem. To make the notation more familiar, we let $q(x = i) = q_i$. Then, we arrive at the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(q, \\lambda) & = & \\sum_i q_i \\log p(y, x=i| \\theta) - \\sum_i q_i \\log q_i \\\\\n",
    "& & + \\lambda (1 - \\sum_i q_i)\n",
    "\\end{eqnarray}\n",
    "We take the derivative with respect to $q_i$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(q, \\lambda)}{\\partial q_k} & = & \\log p(y, x=k| \\theta) - (\\log q_k + 1) - \\lambda = 0\\\\\n",
    "\\log q_k  & = & \\log p(y, x=k| \\theta) -1 - \\lambda \\\\\n",
    "q_k  & = & p(y, x=k| \\theta) \\exp(-1 - \\lambda) \\\\\n",
    "\\sum_k q_k & = & \\exp(-1 - \\lambda) \\sum_k p(y, x=k| \\theta) = 1\\\\\n",
    "\\exp(1 + \\lambda) & = & p(y | \\theta) \\\\\n",
    "\\exp(-1 - \\lambda) & = & 1/p(y | \\theta) \\\\\n",
    "\\end{eqnarray}\n",
    "hence we have\n",
    "\\begin{eqnarray}\n",
    "q_k  & = & p(y, x=k| \\theta)/p(y | \\theta) = p(x=k| y \\theta)\n",
    "\\end{eqnarray}\n",
    "This result shows that the best we can do is to choose the posterior distribution\n",
    "\\begin{eqnarray}\n",
    "q(x) & = & p(x| y, \\theta)\n",
    "\\end{eqnarray}\n",
    "The EM algorithm is an iterative algorithm that exploits this bound result. Given a particular parameter setting $\\theta^{(\\tau)}$ at iteration $\\tau$, we can compute a lower bound of the true likelihood function.\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & \\geq & \\E{\\log {p(y, x| \\theta)}}_{p(x| y, \\theta^{(\\tau)})} -\\E{\\log p(x| y, \\theta^{(\\tau)}) }_{p(x| y, \\theta^{(\\tau)})}\\\\\n",
    "& \\equiv & {\\cal F}[\\theta; \\theta^{(\\tau)}] +  H[p(x| y, \\theta^{(\\tau)})]\n",
    "\\end{eqnarray}\n",
    "We need to show that\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta^{(\\tau)}) & = & \\E{\\log {p(y, x| \\theta)}}_{p(x| y, \\theta^{(\\tau)})} -\\E{\\log p(x| y, \\theta^{(\\tau)}) }_{p(x| y, \\theta^{(\\tau)})}\\\\\n",
    "& = & {\\cal F}[\\theta^{(\\tau)}; \\theta^{(\\tau)}] +  H[p(x| y, \\theta^{(\\tau)})]\n",
    "\\end{eqnarray}\n",
    "In other words, the bound is tight at $\\theta^{(\\tau)}$, hence maximizing the bound guarantees maximization of the true loglikelihoood.\n",
    "\n",
    "In most cases, where the EM algorithm can be applied, the joint distribution is from an {\\it exponential family}, i.e., it has the generic algebraic form\n",
    "\\begin{eqnarray}\n",
    "p(y, x| \\theta) & = & b(y, x)\\exp( \\sum_l \\phi_l(y, x) \\psi(\\theta_l) - A(\\theta)   )\n",
    "\\end{eqnarray}\n",
    "where $\\phi_l$ are the sufficient statistics and $\\psi(\\theta_l)$ are the {\\it canonical} parameters. The canonical parameters are in one to one relation with a conventional parametrisation. We will give several explicit examples when we cover the HMM's of the next section.\n",
    "\n",
    "In the case when the complete data likelihood is an exponential family, the computation of the bound requires the expectation\n",
    "\\begin{eqnarray}\n",
    "\\E{\\log p(y, x| \\theta)} & = & \\E{\\log b(x, y)} + \\sum_l \\E{\\phi_l(y, x)} \\psi(\\theta_l) - A(\\theta)\n",
    "\\end{eqnarray}\n",
    "where the expectation is taken with respect to the posterior $p(x|y, \\theta^{(\\tau)})$. In other words, we need to compute expectations of form $\\E{\\phi_l(y, x)}$. Once these are available, we have effectively an expression for ${\\cal F}(\\theta; \\theta^{(\\tau)})$. By maximisation of ${\\cal F}$ with respect to $\\theta$,  and arrive at $\\theta^{(\\tau + 1)}$ and complete the iteration.\n",
    "\n",
    "In a rather abstract sense, the EM algorithm proceeds as follows:\n",
    "\n",
    "##### The Expectation/Maximization (EM) algorithm.\n",
    "\n",
    "\\begin{algorithmic}\n",
    "\\STATE Initialise $\\theta^{(0)}$\n",
    "\\FOR{  $\\text{epoch} = 1 \\dots $  MAXITER}\n",
    "\\STATE E-step. Compute the sufficient statistics of the complete data likelihood\n",
    "\\STATE M-step. Maximize with respect to the parameters $\\theta$ to find $\\theta^{(\\tau + 1)}$\n",
    "\\ENDFOR\n",
    "\\end{algorithmic}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning HMM parameters by EM\n",
    "\n",
    "Suppose we have observed several sequences\n",
    "\\begin{eqnarray}\n",
    "Y = \\{y_1^{(n)}, y_2^{(n)}, \\dots, y_{T_n}^{(n)}   \\}\n",
    "\\end{eqnarray}\n",
    "for $n = 1\\dots N$. Here $T_n$ is the length of the $n$'th sequence.\n",
    "Let $Y \\in \\{1,\\dots, R\\}$ and $X \\in \\{1,\\dots, S \\}$.\n",
    "\n",
    "The discrete observation, discrete state space HMM has the following factors:\n",
    "\\begin{eqnarray}\n",
    "p(x_1 = i) & = & \\pi_i \\\\\n",
    "p(y_k = r| x_k = i) & = & B_{r,i} \\\\\n",
    "p(x_k = i| x_{k-1} = j) & = & A_{i,j} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The joint probability of all observed sequences and corresponding hidden sequences are\n",
    "\\begin{eqnarray}\n",
    "p(Y, X | \\pi, A, B) & = & \\prod_n \\left( p(x_1^{(n)}) p(y_1^{(n)} | x_1^{(n)})  \\prod_{t=2}^{T_n} p(y_t^{(n)} | x_t^{(n)}) p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right)\n",
    "\\end{eqnarray}\n",
    "The expected complete data loglikelihood is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\\ind{r = y_t^{(n)": "<p><strong>SyntaxError</strong>: unexpected character after line continuation character (<ipython-input-41-609a14242ce0>, line 1)</p>\n",
     "\\ind{s = x_1^{(n)": "<p><strong>SyntaxError</strong>: unexpected character after line continuation character (<ipython-input-41-3849bac311c9>, line 1)</p>\n",
     "\\ind{s = x_t^{(n)": "<p><strong>SyntaxError</strong>: unexpected character after line continuation character (<ipython-input-41-7f095b93201a>, line 1)</p>\n"
    }
   },
   "source": [
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A, B) & = & \\E{\\sum_n \\left( \\log p(x_1^{(n)}) + \\sum_{t=1}^{T_n} \\log p(x_t^{(n)}| x_{t-1}^{(n)} ) + \\sum_{t=2}^{T_n} \\log p(y_t^{(n)} | x_t^{(n)}) \\right)} \\\\\n",
    "& = & \\E{\\sum_n \\left( \\sum_{s=1}^{S} {\\ind{s = x_1^{(n)}}} \\log \\pi_s   + \\log \\pi_s \\sum_{t=2}^{T_n}\\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S} {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} + \\sum_{t=1}^{T_n}\\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} {\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}} \\log B_{r,i}\\right)} \\\\\n",
    "& = & \\sum_n \\sum_{s=1}^{S}   \\E{{\\ind{s = x_1^{(n)}}}} \\log \\pi_s  + \\sum_n \\sum_{t=2}^{T_n}\n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S}   \\E{{\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\log A_{s,j} + \\sum_n \\sum_{t=1}^{T_n}\\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} \\E{{\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}}} \\log B_{r,i} \\\\\n",
    "& = & \\sum_{s=1}^{S} \\underbrace{\\left( \\sum_n \\E{ {\\ind{s = x_1^{(n)}}}} \\right)}_{\\equiv C_{\\pi}} \\log \\pi_s  + \n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S} \\underbrace{\\left(  \\sum_n \\sum_{t=2}^{T_n} \\E{{\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\right)}_{\\equiv C_{A}} \\log A_{s,j} + \\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} \\underbrace{\\left(  \\sum_n \\sum_{t=1}^{T_n} \\E{{\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}}} \\right)}_{\\equiv C_{B}} \\log B_{r,i} \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The M-Step\n",
    "We write the Lagrangian to ensure that the columns of $A$ and $B$ are positive and normalized\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A, \\lambda^B)\n",
    "& = & \\sum_{s=1}^{S} C_{\\pi}(s) \\log \\pi_s + \\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S}  C_{A}(s,j) \\log A_{s,j} + \\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} C_{B}(r,i) \\log B_{r,i} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "+ \\sum_i \\lambda^B_i \\left( 1 - \\sum_r B_{r,i} \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "To find $\\pi$, $A$ and $B$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial \\pi_i} & = & C_{\\pi}(i) \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial A_{i,j}} & = &  C_{A}(i,j) \\frac{1}{A_{i,j}} - \\lambda^A_j = 0 \\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, B, \\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial B_{k,i}} & = &  C_{B}(k,i) \\frac{1}{B_{k,i}} - \\lambda^B_i = 0 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Prior\n",
    "We set the derivative to zero and solve for $\\pi$\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & C_{\\pi}(i) \\frac{1}{\\lambda^\\pi}\n",
    "\\end{eqnarray}\n",
    "As we have the normalization constraint for $\\pi$, we also have the following equality from which we can solve for the Lagrange multiplier:\n",
    "\\begin{eqnarray}\n",
    "\\sum_i \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i C_{\\pi}(i) = 1 \\\\\n",
    "\\lambda^\\pi & = & \\sum_i  C_{\\pi}(i)  = N \n",
    "\\end{eqnarray}\n",
    "Substituting, we obtain the intuitive answer:\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\frac{1}{N} C_{\\pi}(i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Transition Matrix\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & C_A(i,j) \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i C_A(i,j) \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_i C_A(i,j) \\\\\n",
    "A_{i,j} & = &  \\frac{C_A(i,j)}{\\sum_i C_A(i,j)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Observation Matrix\n",
    "\\begin{eqnarray}\n",
    "B_{k,i} & = & C_B(k,i) \\frac{1}{\\lambda^B_i} \\\\\n",
    "\\sum_k B_{k,i} & = & \\sum_k C_B(k,i) \\frac{1}{\\lambda^B_i} = 1 \\\\\n",
    "\\lambda^B_i & = & \\sum_k C_B(k,i) \\\\\n",
    "B_{k,i} & = &  \\frac{C_B(k,i)}{\\sum_k C_B(k,i)}\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:32.171091Z",
     "start_time": "2018-10-12T16:41:32.153454Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HMM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-3714d261ba55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mhm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'HMM' is not defined"
     ]
    }
   ],
   "source": [
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "y = np.array([0, 1, 3, 2, 4])\n",
    "\n",
    "hm = HMM(p, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward smoothers\n",
    "\n",
    "The EM algorithm requires obtaining the sufficient statistics of an HMM.  \n",
    "\n",
    "The key observation is that the sufficient statistics are additive:\n",
    "\\begin{eqnarray}\n",
    "C_t & = & \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k}) \\right) p(x_{1:t}|y_{1:t}) dx_{1:t}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The derivation depends on the following decomposition. By the chain rule of probabilities, we write\n",
    "\\begin{eqnarray}\n",
    "p(x_{1:t}|y_{1:t}) & = & p(x_{1}|x_{2:t}, y_{1:t}) \\cdots p(x_{t-2}|x_{t-1:t}, y_{1:t}) p(x_{t-1}|x_t, y_{1:t}) p(x_{t}|y_{1:t}) \n",
    "\\end{eqnarray}\n",
    "The key observation is that this expression admits computation in the forward direction. By conditional independence\n",
    "\\begin{eqnarray}\n",
    "p(x_{1:t}|y_{1:t}) & = & p(x_{1}|x_{2}, y_{1}) \\cdots p(x_{t-2}|x_{t-1}, y_{1:t-2}) p(x_{t-1}|x_t, y_{1:t-1}) p(x_{t}|y_{1:t}) \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "We will use this observation as the basis of a forward recursion. First we decompose the posterior \n",
    "as a product of the filtering density at time $t$ and a conditional quantity familiar from the correction smoother \n",
    "\\begin{eqnarray}\n",
    "C_t & = & \\int \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k})\\right) p(x_{1:t-1}|y_{1:t},x_t) p(x_{t}|y_{1:t}) dx_{1:t-1} dx_t \\\\\n",
    "& = & \\int \\underbrace{\\left( \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k})\\right) p(x_{1:t-1}|y_{1:t-1},x_t) dx_{1:t-1} \\right)}_{=V_t(x_t)} p(x_{t}|y_{1:t})  dx_t\n",
    "\\end{eqnarray}\n",
    "\n",
    "Due to additivity, we can decompose further\n",
    "\\begin{eqnarray}\n",
    "V_t(x_t) & = & \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-1}|y_{1:t-1}, x_t) dx_{1:t-1} \\\\\n",
    "& = & \\int \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-2}|y_{1:t-2}, x_{t-1}) p(x_{t-1}|y_{1:t-1}, x_t) dx_{1:t-2} dx_{t-1} \\\\\n",
    "& = & \\int \\left( s_t(x_{t-1}, x_{t}) + \\int \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k}) p(x_{1:t-2}|y_{1:t-2}, x_{t-1}) dx_{1:t-2}  \\right)  p(x_{t-1}|y_{1:t-1}, x_t)  dx_{t-1} \\\\\n",
    "& = & \\int \\left( s_t(x_{t-1}, x_{t}) + V_{t-1}(x_{t-1})  \\right)  p(x_{t-1}|y_{1:t-1}, x_t)  dx_{t-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "V_1(x_1) & = & 0 \\\\\n",
    "V_2(x_2) & = & \\int  s_2(x_{1}, x_{2})   p(x_{1}|y_{1}, x_2)  dx_{1} \\\\\n",
    "V_3(x_3) & = & \\int \\left( s_3(x_{2}, x_{3}) + V_{2}(x_{2})  \\right)  p(x_{2}|y_{1:2}, x_3)  dx_{2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\\ind{a = x_2": "<p><strong>SyntaxError</strong>: unexpected character after line continuation character (<ipython-input-42-29f7461bcbd8>, line 1)</p>\n"
    }
   },
   "source": [
    "#### Example \n",
    "Suppose only $y_{1:4}$ are observed. The above recursion, when applied to the sufficient statistics of an HMM has the following specific form. Recall that, $C_{\\pi}, C_A, C_B$ for stand for the sufficient statistics for the estimation of the prior, state transitions  and observations, respectively. Here, we will denote the expected sufficient statistics explicitely as $C_{A,t}$ where $t$ is the number of observations. \n",
    "\n",
    "The state transition statistics are\n",
    "\\begin{eqnarray}\n",
    "C_{A,4}(a,b) &=& \\sum_{x_4} \\sum_{x_3} \\sum_{x_2} \\sum_{x_1} \\left( {\\ind{a = x_2}}\\ind{b = x_{1}} + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) p(x_{1:4}|y_{1,4})\\\\\n",
    "\\end{eqnarray}\n",
    "By introducing the forward decomposition\n",
    "\\begin{eqnarray}\n",
    "C_{A,4}(a,b) &=& \\sum_{x_4} \\sum_{x_3} \\sum_{x_2} \\left(\\sum_{x_1} {\\ind{a = x_2}}\\ind{b = x_{1}} p(x_1|y_1, x_2) + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_2|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4})\\\\\n",
    "&=& \\sum_{x_4}\\sum_{x_3} \\sum_{x_2} \\left( \\underbrace{{\\ind{a = x_2}} p(x_1=b|y_1, x_2)}_{V_{A,2}(x_2)} + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_2|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "&=&  \\sum_{x_4}\\sum_{x_3} \\left( \\underbrace{p(x_1=b|y_1, x_2=a) p(x_2=a|y_{1,2}, x_3) + {\\ind{a = x_3}} p(x_2=b|y_{1,2}, x_3)}_{V_{A,3}(x_3)} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "&=&  \\sum_{x_4} \\left(p(x_1=b|y_1, x_2=a)\\sum_{x_3} p(x_2=a|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) + p(x_2=b|y_{1,2}, x_3=a) p(x_3=a|y_{1,3}, x_4) \\\\\n",
    "+ {\\ind{a = x_4}} p(x_3=b |y_{1,3}, x_4) \\right)  p(x_4|y_{1,4}) \\\\\n",
    "&=& p(x_1=b|y_1, x_2=a)\\sum_{x_3} p(x_2=a|y_{1,2}, x_3) \\sum_{x_4} p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "& & + p(x_2=b|y_{1,2}, x_3=a) \\sum_{x_4} p(x_3=a|y_{1,3}, x_4)p(x_4|y_{1,4}) \\\\\n",
    "& & + p(x_3=b |y_{1,3}, x_4=a) p(x_4=a|y_{1,4})  \n",
    "\\end{eqnarray}\n",
    "\n",
    "One can verify, that the last line is indeed equal to the required sufficient statistics given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_{A,T}(a,b) & = &  \\sum_{x_{1:T}} \\sum_{t=2}^T \\ind{a = x_t}\\ind{b = x_{t-1}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = & \\sum_{x_{2:T}} \\left(\\sum_{x_{1}} \\ind{a = x_2}\\ind{b = x_{1}} p(x_{1}|y_{1},x_{2}) + \\sum_{t=3}^T \\ind{a = x_t}\\ind{b = x_{t-1}}  \\right) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "V_{A,2}(a, b, x_2) & = & \\ind{a = x_2} p(x_{1} = b |y_{1},x_{2}) \\\\\n",
    "C_{A,T}(a,b) & = & \\sum_{x_{3:T}} \\left( \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\sum_{x_2} \\ind{a = x_3}\\ind{b = x_{2}} p(x_{2}|y_{1:2},x_{3})  + \\sum_{t=4}^T \\ind{a = x_t}\\ind{b = x_{t-1}}  \\right) \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "V_{A,3}(a, b, x_3) & = & \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\sum_{x_2} \\ind{a = x_3}\\ind{b = x_{2}} p(x_{2}|y_{1:2},x_{3}) \\\\\n",
    "& = & \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = x_3} p(x_{2}=b|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update rule is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "V_{A,t}(a, b, x_t) & = & \\sum_{x_{t-1}} V_{A,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\sum_{x_{t-1}} \\ind{a = x_t}\\ind{b = x_{t-1}} p(x_{t-1}|y_{1:t-1},x_{t}) \\\\\n",
    "& = & \\sum_{x_{t-1}} V_{A,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\ind{a = x_t} p(x_{t-1}=b|y_{1:t-1},x_{t})\n",
    "\\end{eqnarray}\n",
    "\n",
    "The advantage of this algorithm is that it is entirely forward and has attractive space properties, requiring only $S^3 + 2S$ space. The other statistics are derived below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_{B,T}(a,b) & = &  \\sum_{x_{1:T}} \\sum_{t=1}^T \\ind{a = y_t}\\ind{b = x_{t}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = &  \\sum_{x_{2:T}} \\left(\\ind{a = y_1} p(x_{1}=b|y_{1},x_{2}) + \\sum_{t=2}^T \\ind{a = y_t}\\ind{b = x_{t}} \\right) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{B,2}(a, b, x_2)  & = & \\ind{a = y_1} p(x_{1}=b|y_{1},x_{2}) \\\\\n",
    "C_{B,T}(a,b) & = &  \\sum_{x_{3:T}} \\left(\\sum_{x_2} V_{B,2}(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = y_2} p(x_{2}=b|y_{1:2},x_{3}) + \\sum_{t=3}^T \\ind{a = y_t}\\ind{b = x_{t}} \\right) \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{B,3}(a, b, x_3)  & = & \\sum_{x_2} V_{B,2}(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = y_2} p(x_{2}=b|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update rule is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "V_{B,t}(a, b, x_t) & = & \\sum_{x_{t-1}} V_{B,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\ind{a = y_t} p(x_{t-1}=b|y_{1:t-1},x_{t})\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_{\\pi,T}(a) & = &  \\sum_{x_{1:T}} \\ind{a = x_{1}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = &  \\sum_{x_{2:T}} p(x_{1}=a|y_{1},x_{2}) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{\\pi,2}(a, x_2)  & = & p(x_{1}=a|y_{1},x_{2}) \\\\\n",
    "C_{\\pi,T}(a) & = &  \\sum_{x_{3:T}} \\sum_{x_2} V_{1,2}(a, x_2) p(x_{2}|y_{1:2},x_{3})  \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{\\pi,3}(a, x_3)  & = & \\sum_{x_2} V_{\\pi,2}(a, x_2) p(x_{2}|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update equation is \n",
    "\\begin{eqnarray}\n",
    "V_{\\pi,t}(a, x_t)  & = & \\sum_{x_{t-1}} V_{\\pi,t-1}(a, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " An implementation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:33.349110Z",
     "start_time": "2018-10-12T16:41:33.322048Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HMM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-539c58e29af4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlogB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mhm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HMM' is not defined"
     ]
    }
   ],
   "source": [
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "logA = np.log(A)\n",
    "logB = np.log(B)\n",
    "\n",
    "hm = HMM(p, A, B)\n",
    "\n",
    "y = np.array([1, 1, 1, 3, 2, 0, 3, 2, 1, 0, 0, 3, 2, 4])\n",
    "T = y.shape[0]\n",
    "\n",
    "# Forward only estimation of sufficient statistics\n",
    "V_pi  = np.eye((S))\n",
    "V_A  = np.zeros((S,S,S))\n",
    "V_B  = np.zeros((R,S,S))\n",
    "I_S1S = np.eye(S).reshape((S,1,S))\n",
    "I_RR = np.eye(R)\n",
    "\n",
    "for k in range(T):\n",
    "    if k==0:\n",
    "        log_alpha_pred = np.log(p)\n",
    "    else:\n",
    "        log_alpha_pred = predict(A, log_alpha)\n",
    "    \n",
    "    if k>0:\n",
    "        # Calculate p(x_{k-1}|y_{1:k-1}, x_k) \n",
    "        lp = np.log(normalize_exp(log_alpha)).reshape(S,1) + logA.T    \n",
    "        P = normalize_exp(lp, axis=0)\n",
    "        \n",
    "        # Update\n",
    "        V_pi = np.dot(V_pi, P)             \n",
    "        V_A = np.dot(V_A, P) + I_S1S*P.reshape((1,S,S))    \n",
    "        V_B = np.dot(V_B, P) + I_RR[:,y[k-1]].reshape((R,1,1))*P.reshape((1,S,S))    \n",
    "        \n",
    "    log_alpha = update(y[k], logB, log_alpha_pred)    \n",
    "    p_xT = normalize_exp(log_alpha)    \n",
    "    \n",
    "C1 = np.dot(V_pi, p_xT.reshape(S,1))\n",
    "C2 = np.dot(V_A, p_xT.reshape(1,S,1)).reshape((S,S))\n",
    "C3 = np.dot(V_B, p_xT.reshape(1,S,1)).reshape((R,S))\n",
    "C3[y[-1],:] +=  p_xT\n",
    "    \n",
    "print(\"Results with the Forward Smoother\")\n",
    "print(C1)\n",
    "print(np.sum(C1))\n",
    "\n",
    "print(C2)\n",
    "print(np.sum(C2))\n",
    "\n",
    "print(C3)\n",
    "print(np.sum(C3))\n",
    "\n",
    "print(\"Results with the Correction Smoother\")\n",
    "lg, C1_corr, C2_corr, C3_corr = hm.correction_smoother(y)\n",
    "\n",
    "print(C1_corr)\n",
    "print(np.sum(C1_corr))\n",
    "\n",
    "print(C2_corr)\n",
    "print(np.sum(C2_corr))\n",
    "\n",
    "print(C3_corr)\n",
    "print(np.sum(C3_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T16:41:33.350094Z",
     "start_time": "2018-10-12T16:41:33.326Z"
    }
   },
   "outputs": [],
   "source": [
    "%connect_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
